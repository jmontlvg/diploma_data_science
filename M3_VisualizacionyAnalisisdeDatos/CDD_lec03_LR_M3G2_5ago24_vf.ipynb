{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.random import seed, randn\n",
    "from ipywidgets import interact, IntSlider,FloatSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distintos tipos de variables en análisis de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Variables Categóricas (o Nominales):** Estas variables representan grupos o categorías que no tienen un orden inherente. Por ejemplo, el color de un coche (rojo, azul, verde) o el tipo de animal (perro, gato, pájaro). Las características de estas variables incluyen:\n",
    "   - No tienen un orden lógico.\n",
    "   - Se utilizan para clasificar o agrupar elementos.\n",
    "   - Comúnmente se usan en análisis de frecuencia o para comparar subgrupos.\n",
    "\n",
    "2. **Variables Ordinales:** Son aquellas que, al igual que las categóricas, representan categorías, pero con un orden o jerarquía definido. Un ejemplo sería la calificación en una encuesta (malo, regular, bueno, excelente). Características:\n",
    "   - Tienen un orden claro.\n",
    "   - La diferencia entre categorías no es necesariamente uniforme.\n",
    "   - Se usan para entender rangos o jerarquías, como en encuestas de satisfacción.\n",
    "\n",
    "3. **Variables de Intervalo:** Estas variables numéricas tienen un orden, y la distancia entre cada par de valores es la misma. Un ejemplo  es la temperatura en grados Celsius. Características:\n",
    "   - Tienen un orden y una diferencia constante entre valores.\n",
    "   - No tienen un verdadero cero (0 no significa ausencia del fenómeno).\n",
    "   - Adecuadas para calcular diferencias y promedios.\n",
    "\n",
    "4. **Variables de Razón (o Escala de Razón):** Son similares a las de intervalo, pero con la adición de un punto cero absoluto, lo que significa que 0 indica ausencia total del atributo. Ejemplos incluyen la edad, el ingreso, o el peso. Características:\n",
    "   - Tienen todas las propiedades de las variables de intervalo más un cero absoluto.\n",
    "   - Permiten una gama completa de operaciones estadísticas, incluyendo multiplicaciones y divisiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otros ejemplos de variables de intervalo:\n",
    "\n",
    "1. **Tiempo en un reloj de 12 o 24 horas:** \n",
    "\n",
    "\n",
    "\n",
    "2. **Fechas en el calendario (sin considerar años):** \n",
    "\n",
    "3. **Latitud y longitud geográfica:** Estas medidas indican ubicación, y aunque tienen un orden y una diferencia constante (grados, minutos, segundos), el punto de 0 grados (ya sea latitud o longitud) es un punto de referencia arbitrario, no una ausencia de ubicación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de Regresión Lineal\n",
    "\n",
    "Comencemos con el modelo de regresión lineal simple, donde sólo tenemos una variable explicativa (o predictora) $x$ y una variable respuesta $y$.\n",
    "\n",
    "Nos interesa explicar o predecir $y$ mediante $x$ y **asumimos** que la relación entre ambas es de la forma:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon   \\; \\; \\; \\;...(1)\n",
    "$$\n",
    "\n",
    "donde $\\beta_0$ y $\\beta_1$ son los **coeficientes de regresión** y $\\epsilon$ es un error.\n",
    "\n",
    "Es decir, es una relación lineal y podemos aproximarla mediante una recta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo:\n",
    "\n",
    "1. $ingreso = \\beta_0 + \\beta_1 \\times educacion + \\epsilon$  \n",
    "2. $ingreso = \\beta_0 + \\beta_1 \\times educacion + \\beta_2 \\times $ingreso_padres$ + \\beta_3 \\times salud + \\epsilon$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente, una relación / asociación de este tipo se ve de la siguiente forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"relacion_lineal.png\" alt=\"Alt text\" style=\"width: 600px;\"/>\n",
    "\n",
    "y queremos encontrar una linea que \"represente bien\" esta forma en los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si para las variables $x$ y $y$ tomamos, por ejemplo, 100 observaciones, entonces tendríamos una tabla de datos de la siguiente forma:\n",
    "x|y\n",
    "-|-\n",
    "$x_1$|$y_1$\n",
    "$x_2$|$y_2$\n",
    "$x_3$|$y_3$\n",
    "...|...\n",
    "$x_i$|$y_i$\n",
    "...|...\n",
    "$x_{100}$|$y_{100}$\n",
    "\n",
    "donde $x_i$ es el valor de la variable explicativa (predictora o independiente) del individuo $i$, y $y_i$ el valor de la variable respuesta del individuo $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué quiere decir una línea que represente bien la relación en los datos?\n",
    "\n",
    "\n",
    "+ Queremos aproximar $y$ dado $x$ de la siguiente manera: \n",
    "\n",
    "$$\n",
    " \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x\n",
    "$$\n",
    "\n",
    "donde $\\hat y$ es  nuestro valor estimado (o predicho) y $\\hat\\beta_0$ y $\\hat\\beta_1$ son nuestros coeficiente óptimos (bajo qué métrica??)\n",
    "\n",
    "En este modelo tenemos UNA SOLA variable predictora (feature, 'independent', 'regressor')\n",
    "\n",
    "NOTA: Estamos asumiendo que los datos $y$ 'se generan' de la forma $\\beta_0 + \\beta_1 x + \\epsilon$ , pero desconocemos los verdaderos valores de los coeficientes.\n",
    "\n",
    "Frecuentemente, asumimos además que los errores provienen de una distribución $N(0,\\sigma^{2})$ y que son independientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necesitamos un criterio\n",
    "+ ¿Cómo encontramos  $\\hat\\beta_0$, $\\hat\\beta_1$?\n",
    "+ Un **criterio** puede ser **minimizar la suma de cuadrados de los errores** SSE (sum of squared errors) $\\sum_{i=1}^{n} (\\beta_0 + \\beta_1 x_i - y_i)^2$ o el promedio de este $\\frac{1}{n}SSE = J(\\beta_0, \\beta_1)$\n",
    "+ Considerando $n$ observaciones:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    J(\\beta_0, \\beta_1)  &= \\frac{1}{n}\\sum_{i=1}^{n} (\\beta_0 + \\beta_1 x_i - y_i)^2 \\\\\n",
    "                          &= \\frac{1}{n} \\left[(\\beta_0 + \\beta_1 x_1 - y_1)^2 + (\\beta_0 + \\beta_1 x_2 - y_2)^2 + \\ldots + (\\beta_0 + \\beta_1 x_{n} - y_{n})^2\\right]\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "+ Nos gustaría minimizar $J(\\beta_0, \\beta_1)$ el cual representa un error cuadrático medio o MSE (Mean Squared Error) y por tanto mide las desviaciones al cuadrado entre nuestras predicciones (aún no hechas) y la observación real\n",
    "+ A este criterio se le conoce como **MÍNIMOS CUADRADOS ORDINARIOS (MCO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Un ejemplo visual que nos ayuda a entender esto de una mejor forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(yhat, y): return np.sum((yhat -y)**2)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotJ(beta0,beta1):\n",
    "    seed(31415) #para reproducibilidad\n",
    "    x = np.array([1,1.5,2,3,4,5,5.5]) #7 observaciones\n",
    "    y = .3 +1.5*x +randn(len(x)) # como se genera la y = 0.3 + 1.4x +error\n",
    "    xrange = np.linspace(min(x), max(x), 100)\n",
    "    yhat = beta0 + beta1*xrange\n",
    "    plt.scatter(x,y)\n",
    "    plt.plot(xrange, yhat, color = 'crimson')\n",
    "    plt.ylim(0,12)\n",
    "    plt.title(r\" (1/n)SSE = $J(\\beta_0,\\beta_1)=$\"+ format(J(beta0+x*beta1,y),\".2f\"))\n",
    "    for xi, yi in zip(x,y):\n",
    "        pred = beta0 +beta1*xi\n",
    "        plt.vlines(xi,yi,pred)\n",
    "        plt.text(xi+.1, yi, round((yi-pred)**2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301b27972996499997547cf86f75306e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='beta0', max=2.0, min=-1.0), FloatSlider(value=0.0, d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#interactivo\n",
    "_=interact(plotJ,\n",
    "        beta0 = FloatSlider(min=-1, max=2, step =.1),\n",
    "        beta1 = FloatSlider(min=-1, max=2, step =.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\beta_0, \\beta_1)$ representa una región con forma de paraboloide:\n",
    "\n",
    "\n",
    "<img src=\"paraboloid_cost.png\" alt=\"Alt text\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un dataset (x,y) con $n$ observaciones, puede demostrarse que los valores $\\beta_0^*$, $\\beta_1^*$ que minimizan $J(\\beta_0, \\beta_1)$ son: \n",
    "$$\n",
    "    \\beta_1^* = \\frac{n\\sum_{i=1}^nx_iy_i - \\left(\\sum_{i=1}^nx_i\\right)\\left(\\sum_{i=1}^ny_i\\right)}{n\\sum_{i=1}^nx_i^2 - \\left(\\sum_{i=1}^nx_i\\right)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\beta_0^* = \\frac{1}{n}\\sum_{i=1}^ny_i - \\beta_1^* \\frac{1}{n}\\sum_{i=1}^nx_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pero esta notación no es muy amigable..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Lineal Múltiple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Supongamos que tenemos $n$ observaciones, cada una con $p$ variables predictoras (o features). El modelo en este caso es:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + + \\beta_p x_p + \\epsilon   \\; \\; \\; \\;...(2)\n",
    "$$\n",
    "\n",
    "\n",
    "Tomando $n$ observaciones, donde $x_{i,j}$ denota la $j$-ésima característica o variable de la $i$-ésima observación o individuo, \n",
    "el problema sería encontrar $\\hat\\beta_0, \\hat\\beta_1, \\ldots, \\hat\\beta_p$. Nuestra predicción $\\hat y_i$ para la $i$-ésima observación  estaría dada por\n",
    "$$\n",
    "    \\begin{align}\n",
    "    \\hat y_i &= \\hat\\beta_0 + \\hat\\beta_1 x_{i1} +  \\hat\\beta_2 x_{i2} + \\ldots + \\hat\\beta_p x_{ip}\\\\\n",
    "            &= \\hat\\beta^{\\prime} x_i\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde\n",
    "$$\n",
    "   \\hat\\beta = \\begin{bmatrix}\n",
    "                \\hat\\beta_0 \\\\\n",
    "                \\hat\\beta_1 \\\\\n",
    "                \\hat\\beta_2 \\\\\n",
    "                \\vdots  \\\\\n",
    "                \\hat\\beta_p\n",
    "             \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    x_i = \\begin{bmatrix}\n",
    "                1, \n",
    "                x_{i1}, \n",
    "                x_{i2},\n",
    "                \\ldots, \n",
    "                x_{ip}\n",
    "             \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo encontramos $\\hat\\beta$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En términos matriciales, si $$\n",
    "    y = \\begin{bmatrix} \n",
    "                y_{1}, \n",
    "                y_{2},\n",
    "                \\ldots, \n",
    "                y_{n}\n",
    "             \\end{bmatrix}^{\\prime}\n",
    "$$\n",
    "\n",
    "contiene los valores de la variable respuesta para los individuos, y $X$ la matriz de $p$ variables explicativas o predictoras para los $n$ individuos:\n",
    "\n",
    "$$\n",
    "    X = \\begin{bmatrix}\n",
    "        – x_1 – \\\\\n",
    "        – x_2 – \\\\\n",
    "        \\vdots \\\\\n",
    "        – x_n – \\\\\n",
    "        \\end{bmatrix}\n",
    "        = \\begin{bmatrix}\n",
    "                1 & x_{1,1} & x_{1,2} & \\ldots & x_{1,p}\\\\\n",
    "                1 & x_{2,1} & x_{2,2} & \\ldots & x_{2,p}\\\\\n",
    "                \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "                1 & x_{n,1} & x_{n,2} & \\ldots & x_{n,p}\\\\\n",
    "          \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, el **modelo de regresión lineal múltiple** puede representarse como:\n",
    "\n",
    "$$\n",
    "y = X\\beta + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde\n",
    "$$\n",
    "    \\epsilon = \\begin{bmatrix} \n",
    "                \\epsilon_{1}, \n",
    "                \\epsilon_{2},\n",
    "                \\ldots, \n",
    "                \\epsilon_{n}\n",
    "             \\end{bmatrix}^{\\prime}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\beta = \\begin{bmatrix}\n",
    "                \\beta_{0},\n",
    "                \\beta_{1}, \n",
    "                \\beta_{2},\n",
    "                \\ldots, \n",
    "                \\beta_{p}\n",
    "             \\end{bmatrix}^{\\prime}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y el criterio de **mínimos cuadrados** nos indica minimizar:\n",
    "\n",
    "$$\n",
    "J(\\beta_0, \\beta_1) = SSE = \\epsilon^{\\prime}\\epsilon = (y - X\\beta)^{\\prime}(y - X\\beta) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El estimador de Mínimos Cuadrados Ordinarios (MCO) se deriva minimizando la suma de los errores cuadráticos entre los valores predichos y los valores reales de la variable dependiente,  y está dado por:\n",
    "\n",
    "$$\n",
    "\\hat\\beta = (X^{\\prime}X)^{-1}X^{\\prime}y\n",
    "$$\n",
    "\n",
    "donde $(X^{\\prime}X)^{-1}$ es la inversa del producto matricial $X^{\\prime}X$, y $X^{\\prime}$ es la transpuesta de la matriz $X$.\n",
    "\n",
    "Para derivar este estimador, comenzamos escribiendo el modelo de regresión lineal como:\n",
    "\n",
    "$$\n",
    "y = X\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "donde $e$ es un vector columna de $n \\times 1$ de los errores. Nuestro objetivo es encontrar los valores de $b$ que minimicen la suma de los errores cuadrados:\n",
    "\n",
    "$$\n",
    "SSE = \\epsilon^{\\prime}\\epsilon = (y - X\\beta)^{\\prime}(y - X\\beta) \n",
    "                  = y^{\\prime}y -\\beta^{\\prime}X^{\\prime}y - y^{\\prime}X\\beta + \\beta^{\\prime}X^{\\prime}X\\beta\n",
    "$$\n",
    "\n",
    "Tomando la derivada de $SSE$ con respecto a $b$ e igualándola a cero, obtenemos:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial SSE}{\\partial \\beta} = -2X^{\\prime}y + 2X^{\\prime}X\\hat\\beta = 0\n",
    "$$\n",
    "\n",
    "Resolviendo para $\\hat\\beta$, obtenemos:\n",
    "\n",
    "$$\n",
    "\\hat\\beta = (X^{\\prime}X)^{-1}X^{\\prime}y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas propiedades del estimador MCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que los errores $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. El estimador es insesgado:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{E}(\\hat{\\beta}) &= \\mathrm{E}((X^T X)^{-1} X^T y) = (X^T X)^{-1} X^T \\mathrm{E}(y) = (X^T X)^{-1} X^T \\mathrm{E}(X\\beta + \\epsilon) = (X^T X)^{-1} X^TX \\mathrm{E}(\\beta + \\epsilon) = \\beta\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Su varianza está dada por $\\sigma^2 (X^T X)^{-1}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{cov}(\\hat{\\beta}) &= \\mathrm{var}((X^T X)^{-1} X^T y) = (X^T X)^{-1} X^T \\mathrm{var}(y)X(X^T X)^{-1} \\\n",
    "= (X^T X)^{-1} X^T \\mathrm{var}(\\epsilon) X(X^T X)^{-1} \\\n",
    "= (X^T X)^{-1} X^T \\sigma^2 I_n X (X^T X)^{-1} \\  \n",
    "= \\sigma^2 (X^T X)^{-1}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teorema: Gauss -Markov : \n",
    "# bAJO los supuestos del modelo de regresión lineal múltiple, el estimador de MCO es, de entre todos los estimadores insesgados\n",
    "# de beta, (que son c.lineal de las observaciones y), el de mínima varianza. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Los **valor predichos** $\\hat{y}$ se calculan como $\\hat{y} = X \\hat\\beta $\n",
    "+ Notemos que:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} = X \\hat\\beta &= X(X^{\\prime}X)^{-1}X^{\\prime}y\\\\\n",
    "                    &= Py\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde $P = X(X^{\\prime}X)^{-1}X^{\\prime}$ es la matriz de **proyección ortogonal** sobre el el espacio columna de $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se puede probar directamente que P' = P , porque P' = X(X'X)^(-1)X' = P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"proy_ortogonal_mco.png\" alt=\"Alt text\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que la proyeción ortogonal de un vector $\\mathbf{z}$ sobre un vector $\\mathbf{w}$ es:\n",
    "\n",
    "$$\\mathrm{proj}_{\\mathbf{w}}(\\mathbf{z}) = \\frac{\\mathbf{z} \\cdot \\mathbf{w}}{|\\mathbf{w}|^2} \\mathbf{w},$$\n",
    "\n",
    "donde $\\cdot$ denota el producto interior (producto punto en este caso) y $|\\cdot|$ denota la norma Euclidiana. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos lo anterior para el caso de una regresión simple. Entonces, nuestra matriz de predictoras es:\n",
    "$$\n",
    "  X = \\begin{bmatrix}\n",
    "        x_1 \\\\\n",
    "        x_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        x_n \\\\\n",
    "        \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "y las observaciones de la variables respuesta son:\n",
    "$$\n",
    "  y = \\begin{bmatrix}\n",
    "        y_1 \\\\\n",
    "        y_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        y_n \\\\\n",
    "        \\end{bmatrix}\n",
    "$$\n",
    "Además:\n",
    "$$\n",
    " X^TX = \\sum_i x_{i}^2 = ||X||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores estimados $\\hat y$ está dado por $X\\hat\\beta$, de modo que:\n",
    "\n",
    "$$\n",
    "\\hat y = X\\hat\\beta = X\\frac{1}{||X||^2}X^Ty = \\frac{y^TX}{||X||^2}X = \\frac{y\\cdot X}{||X||^2}X = \\mathrm{proj}_{X}(y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
